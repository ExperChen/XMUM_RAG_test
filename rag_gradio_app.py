"""
RAGç³»ç»Ÿçš„Gradioå¯è§†åŒ–ç•Œé¢
åŠŸèƒ½ï¼šæ–‡ä»¶ä¸Šä¼ ã€æ–‡æ¡£åˆ‡ç‰‡æŸ¥çœ‹ã€æ™ºèƒ½é—®ç­”
"""

import gradio as gr
import os
import tempfile
from langchain_unstructured import UnstructuredLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.vectorstores.utils import filter_complex_metadata
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from config import get_langchain_model
import warnings

# è¿‡æ»¤è­¦å‘Šä¿¡æ¯
warnings.filterwarnings("ignore", category=FutureWarning)

# å…¨å±€å˜é‡å­˜å‚¨å½“å‰çš„RAGç³»ç»Ÿç»„ä»¶
current_vectorstore = None
current_qa_chain = None
current_splits = []

def process_uploaded_file(file_path, chunk_size, chunk_overlap, split_method):
    """
    å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼šåŠ è½½ã€åˆ‡ç‰‡ã€åˆ›å»ºå‘é‡æ•°æ®åº“
    
    å‚æ•°:
        file_path: ä¸Šä¼ æ–‡ä»¶çš„è·¯å¾„
        chunk_size: åˆ‡ç‰‡å¤§å°
        chunk_overlap: åˆ‡ç‰‡é‡å å¤§å°
        split_method: åˆ‡ç‰‡æ–¹æ³•ï¼ˆ"æŒ‰å­—ç¬¦æ•°" æˆ– "æŒ‰æ®µè½"ï¼‰
    
    è¿”å›:
        å¤„ç†çŠ¶æ€ä¿¡æ¯å’Œåˆ‡ç‰‡å†…å®¹
    """
    global current_vectorstore, current_qa_chain, current_splits
    
    try:
        if file_path is None:
            return "è¯·å…ˆä¸Šä¼ æ–‡ä»¶ï¼", ""
        
        # 1. åŠ è½½æ–‡æ¡£
        status = "æ­£åœ¨åŠ è½½æ–‡æ¡£...\n"
        loader = UnstructuredLoader(file_path)
        docs = loader.load()
        status += f"âœ… æˆåŠŸåŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µ\n"
        
        # 2. åˆ‡ç‰‡æ–‡æ¡£
        status += "æ­£åœ¨åˆ‡ç‰‡æ–‡æ¡£...\n"
        
        if split_method == "æŒ‰å­—ç¬¦æ•°":
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size, 
                chunk_overlap=chunk_overlap
            )
        else:  # æŒ‰æ®µè½
            splitter = CharacterTextSplitter(
                separator="\n", 
                chunk_size=chunk_size, 
                chunk_overlap=chunk_overlap
            )
        
        splits = splitter.split_documents(docs)
        current_splits = splits  # ä¿å­˜åˆ‡ç‰‡ç»“æœ
        status += f"âœ… åˆ‡ç‰‡å®Œæˆï¼Œå…±å¾—åˆ° {len(splits)} ä¸ªæ–‡æ¡£ç‰‡æ®µ\n"
        
        # 3. è¿‡æ»¤å…ƒæ•°æ®
        filtered_splits = filter_complex_metadata(splits)
        status += f"âœ… è¿‡æ»¤å…ƒæ•°æ®åæœ‰ {len(filtered_splits)} ä¸ªæœ‰æ•ˆç‰‡æ®µ\n"
        
        # 4. åˆ›å»ºå‘é‡æ•°æ®åº“
        status += "æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“...\n"
        embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        current_vectorstore = Chroma.from_documents(filtered_splits, embedding)
        status += "âœ… å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸï¼\n"
        
        # 5. åˆå§‹åŒ–é—®ç­”é“¾
        status += "æ­£åœ¨åˆå§‹åŒ–é—®ç­”ç³»ç»Ÿ...\n"
        llm = get_langchain_model()
        current_qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            retriever=current_vectorstore.as_retriever(),
            return_source_documents=True
        )
        status += "âœ… é—®ç­”ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼\n\nğŸ‰ æ–‡æ¡£å¤„ç†å®Œæˆï¼Œç°åœ¨å¯ä»¥å¼€å§‹æé—®äº†ï¼"
        
        # ç”Ÿæˆåˆ‡ç‰‡å†…å®¹é¢„è§ˆ
        chunks_preview = generate_chunks_preview(splits)
        
        return status, chunks_preview
        
    except Exception as e:
        error_msg = f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
        return error_msg, ""

def generate_chunks_preview(splits):
    """
    ç”Ÿæˆåˆ‡ç‰‡å†…å®¹çš„é¢„è§ˆæ–‡æœ¬
    
    å‚æ•°:
        splits: æ–‡æ¡£åˆ‡ç‰‡åˆ—è¡¨
    
    è¿”å›:
        æ ¼å¼åŒ–çš„åˆ‡ç‰‡é¢„è§ˆæ–‡æœ¬
    """
    preview = "=== æ–‡æ¡£åˆ‡ç‰‡å†…å®¹é¢„è§ˆ ===\n\n"
    
    for i, split in enumerate(splits[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ªåˆ‡ç‰‡
        preview += f"--- åˆ‡ç‰‡ {i + 1} ---\n"
        preview += f"é•¿åº¦ï¼š{len(split.page_content)} å­—ç¬¦\n"
        preview += f"å†…å®¹ï¼š{split.page_content[:200]}...\n"
        preview += "-" * 50 + "\n\n"
    
    if len(splits) > 10:
        preview += f"... è¿˜æœ‰ {len(splits) - 10} ä¸ªåˆ‡ç‰‡æœªæ˜¾ç¤º ...\n"
    
    return preview

def answer_question(question, show_full_content=True):
    """
    å›ç­”ç”¨æˆ·é—®é¢˜
    
    å‚æ•°:
        question: ç”¨æˆ·æå‡ºçš„é—®é¢˜
        show_full_content: æ˜¯å¦æ˜¾ç¤ºå®Œæ•´çš„æ–‡æ¡£ç‰‡æ®µå†…å®¹
    
    è¿”å›:
        é—®ç­”ç»“æœå’Œç›¸å…³æ–‡æ¡£ç‰‡æ®µ
    """
    global current_qa_chain
    
    if current_qa_chain is None:
        return "âŒ è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£ï¼", ""
    
    if not question.strip():
        return "âŒ è¯·è¾“å…¥é—®é¢˜ï¼", ""
    
    try:
        # è°ƒç”¨é—®ç­”é“¾
        response = current_qa_chain.invoke({"query": question})
        
        # è·å–å›ç­”
        answer = response['result']
        
        # è·å–ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
        source_docs = ""
        if 'source_documents' in response and response['source_documents']:
            source_docs = "\n=== ç›¸å…³æ–‡æ¡£ç‰‡æ®µ ===\n\n"
            for i, doc in enumerate(response['source_documents'][:3]):
                content = doc.page_content
                
                if show_full_content:
                    # æ˜¾ç¤ºå®Œæ•´å†…å®¹
                    source_docs += f"ç‰‡æ®µ {i+1}ï¼ˆé•¿åº¦ï¼š{len(content)} å­—ç¬¦ï¼‰ï¼š\n"
                    source_docs += f"{content}\n\n"
                else:
                    # æ˜¾ç¤ºæˆªæ–­å†…å®¹
                    source_docs += f"ç‰‡æ®µ {i+1}ï¼š\n{content[:300]}...\n\n"
                
                source_docs += "-" * 50 + "\n\n"
        
        return answer, source_docs
        
    except Exception as e:
        error_msg = f"âŒ å›ç­”é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
        return error_msg, ""

def view_all_chunks():
    """
    æŸ¥çœ‹æ‰€æœ‰åˆ‡ç‰‡å†…å®¹
    
    è¿”å›:
        å®Œæ•´çš„åˆ‡ç‰‡å†…å®¹
    """
    global current_splits
    
    if not current_splits:
        return "âŒ æ²¡æœ‰å¯æ˜¾ç¤ºçš„åˆ‡ç‰‡å†…å®¹ï¼Œè¯·å…ˆä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£ï¼"
    
    full_content = "=== å®Œæ•´åˆ‡ç‰‡å†…å®¹ ===\n\n"
    
    for i, split in enumerate(current_splits):
        full_content += f"--- åˆ‡ç‰‡ {i + 1} ---\n"
        full_content += f"é•¿åº¦ï¼š{len(split.page_content)} å­—ç¬¦\n"
        full_content += f"å†…å®¹ï¼š\n{split.page_content}\n"
        full_content += "=" * 60 + "\n\n"
    
    return full_content

# åˆ›å»ºGradioç•Œé¢
def create_interface():
    """
    åˆ›å»ºGradioç”¨æˆ·ç•Œé¢
    """
    with gr.Blocks(title="RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ", theme=gr.themes.Soft()) as app:
        gr.Markdown(
            """
            # ğŸ“š RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ
            
            æ¬¢è¿ä½¿ç”¨åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼
            
            ## ä½¿ç”¨æ­¥éª¤ï¼š
            1. ğŸ“ ä¸Šä¼ æ–‡æ¡£æ–‡ä»¶ï¼ˆæ”¯æŒPDFã€TXTã€DOCXç­‰æ ¼å¼ï¼‰
            2. âš™ï¸ è®¾ç½®åˆ‡ç‰‡å‚æ•°å¹¶å¤„ç†æ–‡æ¡£
            3. ğŸ‘€ æŸ¥çœ‹æ–‡æ¡£åˆ‡ç‰‡å†…å®¹
            4. â“ å‘ç³»ç»Ÿæé—®ï¼Œè·å¾—åŸºäºæ–‡æ¡£çš„æ™ºèƒ½å›ç­”
            """
        )
        
        with gr.Tab("ğŸ“ æ–‡æ¡£ä¸Šä¼ ä¸å¤„ç†"):
            with gr.Row():
                with gr.Column(scale=1):
                    # æ–‡ä»¶ä¸Šä¼ ç»„ä»¶
                    file_upload = gr.File(
                        label="ä¸Šä¼ æ–‡æ¡£æ–‡ä»¶",
                        file_types=[".pdf", ".txt", ".docx", ".md"],
                        type="filepath"
                    )
                    
                    # åˆ‡ç‰‡å‚æ•°è®¾ç½®
                    gr.Markdown("### âš™ï¸ åˆ‡ç‰‡å‚æ•°è®¾ç½®")
                    chunk_size = gr.Slider(
                        minimum=100, 
                        maximum=2000, 
                        value=500, 
                        step=50,
                        label="åˆ‡ç‰‡å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰"
                    )
                    chunk_overlap = gr.Slider(
                        minimum=0, 
                        maximum=200, 
                        value=50, 
                        step=10,
                        label="åˆ‡ç‰‡é‡å ï¼ˆå­—ç¬¦æ•°ï¼‰"
                    )
                    split_method = gr.Radio(
                        choices=["æŒ‰å­—ç¬¦æ•°", "æŒ‰æ®µè½"],
                        value="æŒ‰å­—ç¬¦æ•°",
                        label="åˆ‡ç‰‡æ–¹æ³•"
                    )
                    
                    # å¤„ç†æŒ‰é’®
                    process_btn = gr.Button("ğŸš€ å¤„ç†æ–‡æ¡£", variant="primary")
                
                with gr.Column(scale=2):
                    # å¤„ç†çŠ¶æ€æ˜¾ç¤º
                    process_status = gr.Textbox(
                        label="å¤„ç†çŠ¶æ€",
                        lines=10,
                        interactive=False
                    )
        
        with gr.Tab("ğŸ‘€ åˆ‡ç‰‡å†…å®¹æŸ¥çœ‹"):
            with gr.Row():
                view_chunks_btn = gr.Button("ğŸ“‹ æŸ¥çœ‹æ‰€æœ‰åˆ‡ç‰‡", variant="secondary")
            
            chunks_display = gr.Textbox(
                label="æ–‡æ¡£åˆ‡ç‰‡å†…å®¹",
                lines=20,
                interactive=False,
                show_copy_button=True
            )
        
        with gr.Tab("â“ æ™ºèƒ½é—®ç­”"):
            with gr.Row():
                with gr.Column(scale=2):
                    question_input = gr.Textbox(
                        label="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜",
                        placeholder="ä¾‹å¦‚ï¼šè¿™ä»½æ–‡æ¡£ä¸»è¦è®²äº†ä»€ä¹ˆï¼Ÿ",
                        lines=3
                    )
                    ask_btn = gr.Button("ğŸ¤” æé—®", variant="primary")
                
                with gr.Column(scale=3):
                    answer_output = gr.Textbox(
                        label="AIå›ç­”",
                        lines=10,
                        interactive=False,
                        show_copy_button=True
                    )
            
            source_docs_output = gr.Textbox(
                label="ç›¸å…³æ–‡æ¡£ç‰‡æ®µ",
                lines=8,
                interactive=False,
                show_copy_button=True
            )
        
        # ç»‘å®šäº‹ä»¶
        process_btn.click(
            fn=process_uploaded_file,
            inputs=[file_upload, chunk_size, chunk_overlap, split_method],
            outputs=[process_status, chunks_display]
        )
        
        view_chunks_btn.click(
            fn=view_all_chunks,
            outputs=chunks_display
        )
        
        ask_btn.click(
            fn=answer_question,
            inputs=question_input,
            outputs=[answer_output, source_docs_output]
        )
        
        # æ”¯æŒå›è½¦é”®æé—®
        question_input.submit(
            fn=answer_question,
            inputs=question_input,
            outputs=[answer_output, source_docs_output]
        )
    
    return app

# å¯åŠ¨åº”ç”¨
if __name__ == "__main__":
    app = create_interface()
    app.launch(
        server_name="127.0.0.1",  # å…è®¸å¤–éƒ¨è®¿é—®
        server_port=7860,       # ç«¯å£å·
        share=True,            # æ˜¯å¦åˆ›å»ºå…¬å…±é“¾æ¥
        debug=True              # è°ƒè¯•æ¨¡å¼
    )